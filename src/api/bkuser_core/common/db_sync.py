# -*- coding: utf-8 -*-
"""
TencentBlueKing is pleased to support the open source community by making è“é²¸æ™ºäº‘-ç”¨æˆ·ç®¡ç†(Bk-User) available.
Copyright (C) 2017-2021 THL A29 Limited, a Tencent company. All rights reserved.
Licensed under the MIT License (the "License"); you may not use this file except in compliance with the License.
You may obtain a copy of the License at http://opensource.org/licenses/MIT
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"""
import logging
import math
from dataclasses import dataclass, field
from enum import auto
from threading import RLock
from typing import Any, ClassVar, List, Optional, Type

from django.db import connections, models

from bkuser_core.common.enum import AutoLowerEnum

logger = logging.getLogger(__name__)


class SyncOperation(AutoLowerEnum):
    ADD = auto()
    UPDATE = auto()


class SyncModelMeta:
    """
    ä¸»è¦æ˜¯é’ˆå¯¹ä¸åŒçš„åç«¯åŒæ­¥ç³»ç»Ÿçš„åŒæ­¥DBæ¨¡å‹æè¿°

    ä¾‹å¦‚ï¼Œ
    excel å¯¼å…¥æ•°æ®æ—¶ï¼Œæ˜¯ç”¨ username è€Œä¸æ˜¯ code ä½œä¸ºå”¯ä¸€æ ‡è¯†
    ldap or mad æ˜¯æœ‰ uuid ä½œä¸º code çš„
    """

    target_model: ClassVar[Type[models.Model]]
    table_name: ClassVar[str]
    is_relation_table: bool = False
    pk_field: str = "id"
    update_exclude_fields: List = []
    use_bulk: bool = True
    # TODO: support unique_together
    unique_key_field: ClassVar[str] = ""
    sharding_size: int = 1000

    @classmethod
    def has_unique_key(cls) -> bool:
        return bool(cls.unique_key_field)


@dataclass
class SyncModelManager:
    """åŒæ­¥DBæ¨¡å‹ç®¡ç†å™¨

    èšç„¦äºå°†å†…å­˜å¯¹è±¡åŒæ­¥åˆ° DB, æ”¯æŒæ’å…¥ & æ›´æ–°
    """

    meta: Type[SyncModelMeta]

    # too big sql may cause "mysql gone away"
    adding_items: list = field(default_factory=list)
    updating_items: list = field(default_factory=list)
    _action_map_cache: dict = field(default_factory=dict)

    _default_sync_operation = SyncOperation.ADD.value
    _lock = RLock()

    def get(self, unique_key: Any) -> Optional[Any]:
        if not self.meta.has_unique_key():
            raise ValueError(f"there is no 'unique_key_field' in model meta<{self.meta}>, cannot get certain item")

        return self._action_map_cache.get(unique_key)

    def exists(self, db_obj: models.Model) -> bool:
        """æ˜¯å¦åœ¨è¡Œä¸ºç¼“å­˜ä¸­å·²å­˜åœ¨æŸä¸ªèµ„æº"""
        if not self.meta.has_unique_key():
            raise ValueError(f"there is no 'unique_key_field' in model meta<{self.meta}>, cannot get certain item")

        return getattr(db_obj, self.meta.unique_key_field) in self._action_map_cache

    def add(self, db_obj: models.Model, operation: SyncOperation = None) -> None:
        """å¢åŠ ä¸€ä¸ªè¡Œä¸º(Action)ç¼“å­˜åˆ°é˜Ÿåˆ—

        ä¸€ä¸ª DB å¯¹è±¡ï¼ŒåŠ ä¸€ä¸ªæ“ä½œï¼Œç»„æˆä¸€ä¸ªè¡Œä¸º Action
        """
        operation = operation or self._default_sync_operation

        if not isinstance(db_obj, self.meta.target_model):
            raise ValueError(f"db_obj<{db_obj}> is not a {self.meta.target_model}")

        _cache_key = None
        if self.meta.unique_key_field:
            unique_key = getattr(db_obj, self.meta.unique_key_field)
            if unique_key and unique_key in self._action_map_cache:
                logger.debug("action (%s-%s) already in item cache, skipping", operation, db_obj)
                return

            # å°šä¸å­˜åœ¨ï¼Œæ·»åŠ  unique_key cache
            _cache_key = unique_key

        self._append(db_obj, operation, _cache_key)

    def _append(self, item: models.Model, operation: SyncOperation = None, cache_key: str = None):
        operation = operation or self._default_sync_operation
        with self._lock:
            if operation == SyncOperation.ADD.value:
                self.adding_items.append(item)
            else:
                self.updating_items.append(item)

            if cache_key:
                self._action_map_cache[cache_key] = item

    ######################
    # ä»¥ä¸‹æ˜¯æ¶‰åŠæ•°æ®åº“çš„æ“ä½œ #
    ######################

    def sync_to_db(self):
        """å°†å†…å­˜å¯¹è±¡åŒæ­¥åˆ°æ•°æ®åº“"""
        if self.meta.use_bulk:
            self._sync_in_batches()
        else:
            self._sync_one_by_one()

    def get_latest_auto_id(self) -> int:
        """æ‰¾åˆ°æœ€å¤§çš„è‡ªå¢ id"""
        with connections["default"].cursor() as cursor:
            cursor.execute(
                "SELECT `AUTO_INCREMENT` "
                "FROM  INFORMATION_SCHEMA.TABLES "
                "WHERE TABLE_NAME = '%s';" % self.meta.table_name
            )
            all_value = cursor.fetchall()
            all_value = [v[0] for v in all_value]
            return max(all_value)

    def make_slices(self, origin_list: List) -> List:
        """å†…å­˜å¯¹è±¡åˆ—è¡¨åˆ‡ç‰‡, é¿å…è§¦å‘ sql å¼‚å¸¸"""
        origin_len = len(origin_list)
        if origin_len < self.meta.sharding_size:
            return [
                origin_list,
            ]

        logger.info("======== Slicing =======")
        slices = []
        count = math.ceil(origin_len / self.meta.sharding_size)
        for i in range(count):
            slices.append(origin_list[self.meta.sharding_size * i : self.meta.sharding_size * (i + 1)])

        return slices

    def _sync_one_by_one(self):
        """å°†å†…å­˜å¯¹è±¡é€ä¸ªå†™å…¥åˆ°æ•°æ®åº“"""
        logger.info(
            "======== Going to adding items<%s> (%s), instead of bulk ========",
            self.meta.target_model.__name__,
            len(self.adding_items),
        )
        for adding in self.adding_items:
            adding.save()

        # å…³ç³»è¡¨ä¸éœ€è¦æ›´æ–°
        if self.meta.is_relation_table:
            return

        logger.info(
            "======== Going to updating items<%s> (%s), instead of bulk ========",
            self.meta.target_model.__name__,
            len(self.updating_items),
        )
        for updating in self.updating_items:
            updating.save()

    def _sync_in_batches(self):
        """ä½¿ç”¨ bulk_create å’Œ bulk_update(åªæœ‰éå…³ç³»è¡¨éœ€è¦æ‰§è¡Œæ›´æ–°æ“ä½œ) å°†å†…å­˜å¯¹è±¡æ‰¹é‡åŒæ­¥åˆ°æ•°æ®åº“"""
        # ä½¿ç”¨ bulk_create å°†å†…å­˜å¯¹è±¡å†™å…¥åˆ°æ•°æ®åº“
        self._sync_adding()

        # å…³ç³»è¡¨ä¸éœ€è¦æ›´æ–°
        if self.meta.is_relation_table:
            return

        # ä½¿ç”¨ bulk_update å°†å†…å­˜å¯¹è±¡çš„å˜æ›´æ‰¹é‡å†™å…¥åˆ°æ•°æ®åº“
        if not self.meta.update_exclude_fields:
            raise ValueError("%s should specific field not updating" % self.meta.target_model)
        self._sync_updating()

    def _sync_adding(self):
        manager = "objects"
        method = "bulk_create"
        items = self.adding_items
        extra_params = {}
        self._sync(items, manager, method, extra_params)

    def _sync_updating(self):
        manager = "update_objects"
        method = "bulk_update"
        items = self.updating_items
        extra_params = {"exclude_fields": self.meta.update_exclude_fields, "pk_field": self.meta.pk_field}
        self._sync(items, manager, method, extra_params)

    def _sync(self, items: List, manager: str, method: str, extra_params: dict):
        target_model_name = self.meta.target_model.__name__
        if not items:
            logger.info("======== %s is empty to %s ğŸ“­ =========", target_model_name, method)
            return

        logger.info("======== Going to %s(count: %s) for %s =========", method, len(items), target_model_name)

        current_count = 0
        total_fail_count = 0
        total_fail_records = []
        slices = self.make_slices(items)
        for idx, part in enumerate(slices):
            logger.info(
                "======== Syncing part of %s(%s/%s) current: %d + %d =========",
                target_model_name,
                idx + 1,
                len(slices),
                current_count,
                len(part),
            )
            current_count = current_count + len(part)
            # NOTE: æ‰¹é‡æ’å…¥å¤±è´¥, ä¼šå¯¼è‡´æ•´ä½“åŒæ­¥ä»»åŠ¡å¤±è´¥
            # - ä¼˜åŒ–: æ‰¹é‡æ’å…¥å¤±è´¥, åˆ‡æ¢æˆå•æ¡æ’å…¥
            # - ä¼˜åŒ–: å•æ¡æ’å…¥å¤±è´¥, continue (ä¼šæ‰“è¯¦ç»†æ—¥å¿—)
            try:
                getattr(getattr(self.meta.target_model, manager), method)(part, **extra_params)
            except Exception:
                logger.warning(
                    "%s %s failed, count=%d, extra_params=%s, will try to sync one by one",
                    target_model_name,
                    method,
                    len(part),
                    extra_params,
                )
                for one in part:
                    try:
                        one.save()
                        continue
                    except Exception:
                        total_fail_count += 1
                        logger.exception(
                            "%s %s: save one by one fail, item=%s, will not be updated, detail=%s",
                            target_model_name,
                            method,
                            one,
                            vars(one),
                        )
                        total_fail_records.append(one)
                        continue
                # åŸå…ˆçš„é€»è¾‘: raise
                # raise
        if total_fail_count > 0:
            logger.error(
                "%s %s failed, total_fail_count=%d, total_fail_records=%s",
                target_model_name,
                method,
                total_fail_count,
                total_fail_records,
            )
            logger.info("======== %s synced. and got %d fail âœ… ========", target_model_name, total_fail_count)
            # TODO: should do something to let the admin know some record fail!
        else:
            logger.info("======== %s synced. âœ… ========", target_model_name)
